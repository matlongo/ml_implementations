{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "In this notebook we are going to take you through the how to implement the **[Iterative Deepening 3 (ID3)](https://en.wikipedia.org/wiki/ID3_algorithm)**. \n",
    ">**Here we should explain what ID3 is and why it is useful**.\n",
    "\n",
    "We will start uploading and cleaning a dataset, and then we will implement a series of very useful functions, which in turn will lead us to the implementation of the *DecisionTree* class. This class will define the three most important functions: fit, predict, and print.\n",
    "\n",
    "So here is the outline we are going to follow:\n",
    "1. Dataset Preprocessing\n",
    "2. Useful Functions Definition\n",
    "3. Using the tree to get predictions\n",
    "4. Comparison with existent libraries\n",
    "5. Research about Decision Trees and their applications\n",
    "\n",
    "## 1. Dataset Preprocessing\n",
    "\n",
    "We will first start by uploading the dataset on which we are going to work. For that purpose we are going to work with *[pandas](https://pandas.pydata.org/)* which is very handy to upload and process dataset files. Let's get started by uploading the file, and taking a look to it.\n",
    "\n",
    "The dataset is composed of the following features:\n",
    "- How densely the place is usually **occupied** {High, Moderate, Low}\n",
    "- How the **prices** are {Expensive, Normal, Cheap}\n",
    "- Volume of the **music** {Loud, Quiet}\n",
    "- The **location** {Talpiot, City-Center, Mahane-Yehuda, Ein-Karem, German-Colony}\n",
    "- Whether you are a frequent customer (**VIP**) {Yes, No}\n",
    "- Whether this place has your **favorite beer** {Yes, No}\n",
    "- Whether you **enjoyed** {Yes, No}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matlongo/Documents/USC/INF 552 - Machine Learning/homework1/'\n",
    "dataset = pd.read_csv(path+'dt-data.txt')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset is quite small (just 22 rows), and only contain 7 features, 6 attributes and one target feature to predict. In our case, the target column to predict is *Enjoy*, and the rest of them are the features we are going to use to train our model, and make predictions.\n",
    "\n",
    "Besides that, we can also note that some of the attribute names look weird (take *Enjoy)* for instance), and so do some of the attribute values, such as *(Occupied* values have a number before the real value. Therefore, we should do some cleaning to the dataset.\n",
    "\n",
    "In this block we will: (1) Strip all the fields; (2) Remove punctutation characters from headers; (3) Remove punctuation characters from cells and also the numbers for the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we remove the parenthesis from the header, as well as the extra spaces\n",
    "dataset.columns = [c.strip().replace('(', '').replace(')', '') for c in dataset.columns]\n",
    "# Now we will remove the punctuation characters from the Enjoy and Occupied columns\n",
    "dataset['Enjoy'] = dataset['Enjoy'].apply(lambda enjoy: enjoy[:-1].strip())\n",
    "dataset['Occupied'] = dataset['Occupied'].apply(lambda occup: occup[4:].strip())\n",
    "# Finally we remove the spaces from all the cells\n",
    "dataset = dataset.applymap(lambda s: s.strip())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now everything is set to start implementing the functions.\n",
    "\n",
    "## 2. Useful Functions Definition\n",
    "\n",
    "Once the dataset is ready to be used, we will move forwards to implement a couple of functions that form the pieces necessary to implement our final algorithm ID3. \n",
    "\n",
    "In order to determine what attribute to use for the split, we are going to use the *Information Gain* metric, which is defined by the following formulae:\n",
    "\n",
    "$Information\\_Gain = Entropy\\_Father-Entropy\\_child$\n",
    "\n",
    "So, we need to calculate the Entropy for a given dataset. The Entropy for a probability distribution $\\vec{p}$ is calculated with the following formulae:\n",
    "\n",
    "$Entropy(\\vec{p}) = -\\sum_{i=1}^{t}p_i*\\log(p_i)$\n",
    "\n",
    "Where *t* is the number of elements in $\\vec{p}$.\n",
    "\n",
    "In our case we want to predict the Entropy for our *target* column. We will call *target* column to the column we will try to estimate, for the shown dataset the target is *Enjoy*. Therefore, to calculate the Entropy for that column, we first need to calculate the probability distribution for each possible value of it:\n",
    "\n",
    "$\\vec{p_{target}} = [\\frac{N_{yes}}{N}, \\frac{N_{no}}{N}]$\n",
    "\n",
    "That is, since *Enjoy* only has two possible values (*Yes* and *No*) the probability distribution for it is going to be the number of rows where *Enjoy* is *Yes* ($N_{yes}$) divided by the total number of rows ($N$), and something similar with the number of rows where *Enjoy* is *No*.\n",
    "\n",
    "We will define a function *get_entropy* that given the dataset, and the target column returns the entropy obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(dataset, target):\n",
    "    \"\"\"\n",
    "    This method returns the entropy for a target column in a given dataset. It basically gets the distribution\n",
    "    of each class, and based on that it calculates the entropy.\n",
    "    - dataset: Pandas DataFrame containing all the dataset.\n",
    "    - target: string representing the dataset's column name for which we want to calculate the entropy.\n",
    "    \n",
    "    Returns a float that represents the target column's entropy, in the given dataset.\n",
    "    \"\"\"\n",
    "    # First we check that the column is in the datataset.\n",
    "    if not(target in dataset):\n",
    "        raise Exception(\"The specified target is not present in the given dataset.\")\n",
    "    # First of all we get the number of occurrences for each class in our target column.\n",
    "    occurrences = dataset[target].value_counts()\n",
    "    # Now we obtain the probability for each class.\n",
    "    p_vector = [float(v) / dataset.shape[0] for v in occurrences.values]\n",
    "    # Finally, we calculate the entropy using the probabilities.\n",
    "    entropy = -sum([p_i * np.log2(p_i) for p_i in p_vector])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now we know how to calculate the entropy for a dataset. As we mentioned at the beginning, *ID3* recursively splits the dataset in sub-datasets based on a particular attribute, until a leaf is obtained, where we will obtain the most popular class. The way to determine what attribute to use to perform the split is by selecting the attribute that gives us the highest Information Gain. So in each level we need to try all the available attributes, perform the split on each of them and calculate the information gain of spliting the dataset using that attribute.\n",
    "\n",
    "Therefore, the next step is to implement a function that returns the Information Gain we would obtain if we were to split the tree by a given parameter. This function has to split the dataset using all the possible values for that given attribute, calculate the Entropy in all the sub-datasets, and calculate the Entropy of that split using the following formulae:\n",
    "> $Gain(S, A) = Entropy(S) - \\sum_{v \\in Values} \\frac{|S_v|}{|S|} * Entropy(S_v)$\n",
    "\n",
    "Where S is the dataset of the previous level, and $S_v$ is the dataset for each value of the branches. Finally, to calculate the Information Gain we simply subtract the entropy of the split to the Entropy of the previous level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_info_gain_for_attr(dataset, prev_entropy, attr, target):\n",
    "    \"\"\"\n",
    "    This function returns the Information Gain value if we were to select attribute attr to split the dataset\n",
    "    in different branches.\n",
    "    - dataset: Pandas DataFrame containing all the dataset.\n",
    "    - prev_entropy: Entropy from the previous level, necessary to calculate the Information gain. Type float.\n",
    "    - attr: Attribute's name to be used for calculating the Information gain.\n",
    "    - target: String representing the dataset's column name for which we want to calculate the entropy and \n",
    "    make the predictions.\n",
    "    \n",
    "    It returns a float representing the Information gain for spliting the dataset using this attribute. Besides,\n",
    "    it also returns a dictionary that contains the entropy for each possible value in attr, and the sub-dataset\n",
    "    corresponding to that value in the column attr.\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    if not(attr in dataset):\n",
    "        raise Exception(\"The specified attr is not present in the given dataset.\")\n",
    "    if not(target in dataset):\n",
    "        raise Exception(\"The specified target is not present in the given dataset.\")\n",
    "    \n",
    "    # First of all we get all the possible values. For example, Yes and No, or High, Moderate and Low.\n",
    "    possible_values = dataset[attr].drop_duplicates().values\n",
    "    \n",
    "    # Now we are going to calculate the entropy for each possible value, and accumulate it in the total_entropy\n",
    "    # variable. Besides that, we are also going to get the portion of the DataFrame that have the specified value\n",
    "    # in the attr column, and remove the attr column for that sub-dataset.\n",
    "    parameters = dict()\n",
    "    total_entropy = 0\n",
    "    for i in possible_values:\n",
    "        # First we get the portion of the dataset that only has the value i.\n",
    "        dataset_i = dataset.set_index(attr).loc[[i]]\n",
    "        # Now we calculate the entropy for this sub-dataset.\n",
    "        entropy_i = get_entropy(dataset_i, target)\n",
    "        # Finally, we add this entropy to the total entropy for this attribute.\n",
    "        total_entropy += float(dataset_i.shape[0])/dataset.shape[0]*entropy_i\n",
    "        parameters[i] = {'dataset': dataset_i, 'entropy': entropy_i}\n",
    "    return prev_entropy - total_entropy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now it is time to define a method to construct the Tree. Before defining such method, we need to define the structure where we will store the each node. Therefore, we came up with *TreeNode* which stores all the variables necessary for each node, that is the name of the attribute selected to split, a dictionary of children, and the class to be predicted. We will suppose that leaf nodes will be called \"Leaf Nodes\", since no attribute is going to be splitted.\n",
    "\n",
    "In addition, we will create the function to build the tree, called *construct_tree*. This function will analyse every feature in a dataset, and calculate the information gain. It will pick the one with the highest information gain to make the split, and then call recursively the function with each of the branches. The approach we use to make the recursions is by using DFS, rather than BFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    This structure will hold all the information for each node we have in the tree: attribute name used \n",
    "    for the split, class to be predicted, and its children.\n",
    "    \"\"\"\n",
    "    def __init__(self, attr_name):\n",
    "        # Constructor with all the initialization\n",
    "        self.children = None\n",
    "        self.name = attr_name\n",
    "        self.class_ = None\n",
    "        self.entropy = 0\n",
    "    \n",
    "    def set_children(self, children):\n",
    "        self.children = children\n",
    "    \n",
    "    def set_class(self, class_):\n",
    "        self.class_ = class_\n",
    "        \n",
    "    def set_entropy(self, entropy):\n",
    "        self.entropy = entropy\n",
    "\n",
    "        \n",
    "def construct_tree(dataset, target, prev_entropy):\n",
    "    \"\"\"\n",
    "    This function will create a Decision Tree given a dataset, and a target column to be predicted \n",
    "    which must be present in the dataset.\n",
    "    - dataset: DataFrame with all the data.\n",
    "    - target: column name that we will use for predictions.\n",
    "    - prev_entropy: Entropy from the previous node.\n",
    "    \n",
    "    Returns a TreeNode with the root of the Decision Tree for the given dataset.\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    if not(target in dataset):\n",
    "        raise Exception(\"The specified target is not present in the given dataset.\")\n",
    "    \n",
    "    # First we get the attributes that we might use for splitting and the distribution of values \n",
    "    # in the target column.\n",
    "    possible_attributes = set(dataset.columns)-{target}\n",
    "    target_counts = dataset[target].value_counts() \n",
    "    # Cut condition: no more attributes or just one possible target result in the DataFrame.\n",
    "    if len(possible_attributes)==0 or target_counts.shape[0]==1:\n",
    "        # We create a Leaf Node.\n",
    "        node = TreeNode(\"Leaf\")\n",
    "        node.set_class(target_counts.argmax())\n",
    "        return node\n",
    "    \n",
    "    # Now we try all the attributes and pick the one with the maximum Information Gain.\n",
    "    max_gain = -1\n",
    "    max_params = None\n",
    "    max_attr = None\n",
    "    for attr in possible_attributes:\n",
    "        gain, params = get_info_gain_for_attr(dataset, prev_entropy, attr, target)\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            max_params = params\n",
    "            max_attr = attr\n",
    "    \n",
    "    # Now we define a node with the attribute with the maximum Information Gain.\n",
    "    node = TreeNode(max_attr)\n",
    "    node.set_class(target_counts.argmax())\n",
    "    node.set_entropy(prev_entropy)\n",
    "    children = dict()\n",
    "    for value, dic in max_params.iteritems():\n",
    "        # For each child we calculate its own sub-tree and append it to our root node.\n",
    "        children[value] = construct_tree(dic['dataset'], target, dic['entropy'])\n",
    "    node.set_children(children)\n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything set, we should be able to run our code and get the Decision Tree. We will use the dataset that we load in Section 1, to try the code. Let's run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_entropy = get_entropy(dataset, 'Enjoy')\n",
    "root = construct_tree(dataset, 'Enjoy', prev_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, now we need a way to print the tree. In the next block we define a function that will print the whole structure of the tree with the following format:\n",
    "```\n",
    "attribute A on the 1st level\n",
    "  first value of attribute A: 1st attribute B1 on the 2nd level\n",
    "    first value of attribute B1: 2st attribute B1 on the 3rd level\n",
    "      ...\n",
    "        n value of attribute X: Leaf\n",
    "          Prediction: Yes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, level=0, prev_attr=\"\"):\n",
    "    if level == 0:\n",
    "        print str(node.name) + \"   (Entropy={})\".format(round(node.entropy, 3))\n",
    "    if node.children:\n",
    "        for attr_value, child_node in node.children.iteritems():\n",
    "            print prev_attr*2 + str(attr_value) + \": \" + str(child_node.name) + \"   (Entropy={})\".format(round(child_node.entropy, 3))\n",
    "            print_tree(child_node, level+1, prev_attr+\"  \")\n",
    "    else:\n",
    "        print prev_attr*2 + \"Prediction: \" + str(node.class_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can obtain our tree for the dataset we sent a couple of bloacks before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks great! Once we have our tree it is time to use it to predict if we are going to enjoy our night out or not. Therefore, we need to define a function which iterates over our tree based on the data given, until we get a leaf and make the prediction it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Using the tree to make predictions\n",
    "\n",
    "Our last method will output a prediction using the datapoints and a tree that receives as an input. What it does is traverse the tree using the given datapoints, and once it reaches a leaf node it returns the corresponding class.\n",
    "\n",
    "We will make a prediction using the following datapoints:\n",
    "\n",
    "{'Occupied': 'Low' , 'Price': 'Expensive' ,  'Music': 'Loud' ,  'Location': 'City-Center', 'VIP': 'No', 'Favorite Beer': 'No' }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node,datapoints):\n",
    "\n",
    "    if (node.name == \"Leaf\"):\n",
    "        return node.class_\n",
    "\n",
    "    attr_value = datapoints[node.name]\n",
    "    if attr_value in node.children:\n",
    "        branch = node.children[attr_value]\n",
    "        return predict(branch,datapoints)\n",
    "    return node.class_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Prediction\n",
    "\n",
    "Now we are going to test how our decission tree predicts. For this task we are going to construct the tree and then call the method predict(node, datapoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_entropy = get_entropy(dataset, 'Enjoy')\n",
    "node = construct_tree(dataset,'Enjoy',prev_entropy)\n",
    "test = {'Occupied': 'Low' , 'Price': 'Expensive' ,  'Music': 'Loud' ,  'Location': 'City-Center', 'VIP': 'No', 'Favorite Beer': 'No' }\n",
    "predict (node, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see the result! The previous output shows the predicted class. For these datapoints, we can visually check in the tree we have generated before, that the prediction is correct. Now, we can use this implementation to make predictions with new datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing to existent libraries\n",
    "In this section we will compare our Decision Tree implementation, with already implemented Decision Trees in Python. In this case we will use the implementation given by *scikit-learn*. First of all we will implement all the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's Decision Tree implementation does not support strings as values for the dataset, only numeric attributes. Therefore, we need to transform our string values to numbers, ie, \"Low\"->0, \"High\"->1 and \"Normal\"->2.\n",
    "\n",
    "Besides, all categorical attributes in the form of 0, 1, 2, ... are going to be considered as a continuous feature rather than categorical, so we need to transform them into *one-hot encoding* where for each category we will have a column in our dataset.\n",
    "\n",
    "To make such changes we will do two preprocessing steps:\n",
    "1. Encode each variable using the LabelEncoder class provided by scikit-learn. This class will transform each string categorical variable into a numeric categorical variable.\n",
    "2. Transform each category from every variable into a column, using the OneHotEncoder class provided by scikit-learn.\n",
    "\n",
    "Once we have that we will obtain the dataset ready to fit the Decision Tree. In addition, regarding the *DecisionTree*'s parameters, we set *'entropy'* so that the metric used by the scikit-learn implementation is the same we used in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "one_hot = OneHotEncoder()\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "X = np.array([encoder.fit_transform(column) for column in dataset.drop('Enjoy', 1).values.T]).T\n",
    "X = one_hot.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to fit the Decision Tree model to our newly defined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(X, dataset['Enjoy'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's see what is the format of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "tree.export_graphviz(cls, out_file='tree.dot')  \n",
    "!dot -Tpng tree.dot -o tree.png\n",
    "\n",
    "plt.figure(figsize=(12,18))\n",
    "plt.axis('off')\n",
    "img=mpimg.imread('tree.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! However, we don't know what each X[*i*] means. Let's see what is the attribute value for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_columns = []\n",
    "columns = dataset.drop('Enjoy', 1).columns\n",
    "ax = 0\n",
    "for j in range(len(columns)):\n",
    "    column = columns[j]\n",
    "    encoder.fit_transform(dataset[column])\n",
    "    for i in range(len(encoder.classes_)):\n",
    "        print \"x[\"+str(ax)+\"]: \"+column+\"_\"+encoder.classes_[i]\n",
    "        ax += 1\n",
    "        new_columns.append(column+\"_\"+encoder.classes_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's compare the tree obtained using *scikit-learn* with the one we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=new_columns)\n",
    "df_X['Enjoy'] = dataset['Enjoy']\n",
    "root = construct_tree(df_X, 'Enjoy', prev_entropy)\n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At the beginning it looks very similar, but after the third step it starts to diverge. The thing is that Scikit implmentation makes further post-processing steps to shrink the size of the tree, that we are not doing in our implementation.\n",
    "\n",
    "# 5. Research about Decision Trees and their applications\n",
    "\n",
    "After doing research about Decision Trees, we found interesting applications in the real world that we would like to describe.\n",
    "\n",
    "In this <a href='https://dash.harvard.edu/bitstream/handle/1/2031718/Predicting%20Individual.pdf?sequence=2'>paper</a>, Craig Silverstein (Stanford University) and Stuart Sheiber (Harvard University) used decision trees to solve the problem that many libraries have when they are trying to order books: since there are a lot of books and the space to keep all them inside the library is unrealistic, librarians have to decide which books to keep inside the library and which books store in off-site locations. Therefore, librarians need to predict library book use somehow.\n",
    "\n",
    "Previous researchers have shown that past use information of books is the most reliable predictor of future use. However, Silverstein and Sheiber found that combining previous use information with bibliographic information it can improve the prediction of future use. With all these information in their hands, their goal was to predict future circulation statistics for individual titles to optimize off-site storage decisions.\n",
    "\n",
    "In the end, applying decision trees to the prediction method developed in the paper, has shown that for libraries like Harvard College Library that uses an off-site storage selection method, the amount of books' movement between library and off-site locations would have been less than a fifth as many per year. This research is important because it can be applied to any other university, industry, company, among others, that need to store as many objects as possible (on-site and off-site) and they need to know how to make decisions about storage.\n",
    "\n",
    "Other research <a href='http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0320(19960501)24:1%3C83::AID-CYTO10%3E3.0.CO;2-R/epdf'>paper</a>, titled 'Methodological Aspects of Using Decision Trees to Characterise Leiomyomatous Tumors', uses decision trees to generate information about the diagnosis and/or prognosis problem of a tumor when it compares and evaluates the information of different types of features for tumor characterization. In other words, for a selected tumor it finds which of the different tumor's groups it belongs and why as well.\n",
    "\n",
    "Moreover, authors of this paper explain that since their objectives were exploratory and directed toward (logical) hypothesis generation (about diagnosis/prognosis problems), decision trees appeared to be more suitable than other classifications techniques. Since decision trees produces explicit logical rules, it is easier to interpret or explain than other classifiers that produce discriminant functions following a linear, piecewise linear, or nonlinear model. Furthermore, this papers states that in recent studies, decision trees have been successfully applied to several complex diagnostic problems.\n",
    "\n",
    "In the end, this paper concludes that the methodology presented will constitute a helpful tool for pathologists who have to deal with complicated problem, like identification of benign versus malignant smooth muscle tumors. It is important to understand that this paper show us how decision trees could help in medicine, an area that every year has so much investigation around the world and the evolution in researches have a really fast pace.\n",
    "\n",
    "Another interesting <a href='https://arxiv.org/pdf/1310.2071.pdf'>application</a> uses decision trees in Education areas. A group of researchers decided to use ID3 and C4.5 algorithms to predict students' performance. The goal was to create an application that, based on historical records of first-year students of engineering, it could predict if a student would pass a class or not in the first year. The study involved analyzing existing data, relevant attributes, preprocessing data, use of machine learning libraries and software development to create a web application that could be used by the staff for predicting admitted student's performance. \n",
    "\n",
    "Results have shown a 75% of accuracy out of 182 students. This study could help academic institutions that receive students with different background and nationalities to improve their education level. At an early stage, an academic institution could pay close attention to students with higher probabilities of failing a class. In addition, outstanding students could be identified as well. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
