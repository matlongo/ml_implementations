{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/home/matlongo/Downloads/homework1/'\n",
    "dataset = pd.read_csv(path+'dt-data.txt')\n",
    "dataset['Enjoy'] = dataset[' Enjoy)'].apply(lambda enjoy: enjoy[:-1].strip())\n",
    "dataset = dataset.drop(' Enjoy)', 1)\n",
    "dataset['Occupied'] = dataset['(Occupied'].apply(lambda occup: occup[4:].strip())\n",
    "dataset = dataset.drop('(Occupied', 1)\n",
    "dataset.columns = [c.strip() for c in dataset.columns]\n",
    "dataset = dataset.applymap(lambda s: s.strip())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_entropy(dataset, target):\n",
    "    \"\"\"\n",
    "    This method returns the entropy for a target column in a given dataset. It basically gets the distribution\n",
    "    of each class, and based on that it calculates the entropy.\n",
    "    - dataset: Pandas DataFrame containing all the dataset.\n",
    "    - target: string representing the dataset's column name for which we want to calculate the entropy.\n",
    "    \n",
    "    Returns a float that represents the target column's entropy, in the given dataset.\n",
    "    \"\"\"\n",
    "    # First we check that the column is in the datataset.\n",
    "    if not(target in dataset):\n",
    "        raise Exception(\"The specified target is not present in the given dataset.\")\n",
    "    # First of all we get the number of occurrences for each class in our target column.\n",
    "    occurrences = dataset[target].value_counts()\n",
    "    # Now we obtain the probability for each class.\n",
    "    p_vector = [float(v) / dataset.shape[0] for v in occurrences.values]\n",
    "    # Finally, we calculate the entropy using the probabilities.\n",
    "    entropy = -sum([p_i * np.log(p_i) for p_i in p_vector])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_info_gain_for_attr(dataset, prev_entropy, attr, target):\n",
    "    \"\"\"\n",
    "    This function returns the Information Gain value if we were to select attribute attr to split the dataset\n",
    "    in different branches.\n",
    "    - dataset: Pandas DataFrame containing all the dataset.\n",
    "    - prev_entropy: Entropy from the previous level, necessary to calculate the Information gain. Type float.\n",
    "    - attr: Attribute's name to be used for calculating the Information gain.\n",
    "    - target: String representing the dataset's column name for which we want to calculate the entropy and \n",
    "    make the predictions.\n",
    "    \n",
    "    It returns a float representing the Information gain for spliting the dataset using this attribute. Besides,\n",
    "    it also returns a dictionary that contains the entropy for each possible value in attr, and the sub-dataset\n",
    "    corresponding to that value in the column attr.\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    if not(attr in dataset):\n",
    "        raise Exception(\"The specified attr is not present in the given dataset.\")\n",
    "    if not(target in dataset):\n",
    "        raise Exception(\"The specified target is not present in the given dataset.\")\n",
    "    \n",
    "    # First of all we get all the possible values. For example, Yes and No, or High, Moderate and Low.\n",
    "    possible_values = dataset[attr].drop_duplicates().values\n",
    "    \n",
    "    # Now we are going to calculate the entropy for each possible value, and accumulate it in the total_entropy\n",
    "    # variable. Besides that, we are also going to get the portion of the DataFrame that have the specified value\n",
    "    # in the attr column, and remove the attr column for that sub-dataset.\n",
    "    parameters = dict()\n",
    "    total_entropy = 0\n",
    "    for i in possible_values:\n",
    "        # First we get the portion of the dataset that only has the value i.\n",
    "        dataset_i = dataset.set_index(attr).loc[[i]]\n",
    "        # Now we calculate the entropy for this sub-dataset.\n",
    "        entropy_i = get_entropy(dataset_i, target)\n",
    "        # Finally, we add this entropy to the total entropy for this attribute.\n",
    "        total_entropy += float(dataset_i.shape[0])/dataset.shape[0]*entropy_i\n",
    "        parameters[i] = {'dataset': dataset_i, 'entropy': entropy_i}\n",
    "    return prev_entropy - total_entropy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_attr_to_split(dataset, target, prev_entropy):\n",
    "    \n",
    "    possible_attributes = set(dataset.columns)-{target}\n",
    "    target_counts = dataset[target].value_counts() \n",
    "    # Cut condition\n",
    "    print target_counts\n",
    "    if len(possible_attributes)==0 or target_counts.shape[0]==1:\n",
    "        node = TreeNode(\"Leaf\")\n",
    "        node.set_class(target_counts.argmax())\n",
    "        return node\n",
    "    \n",
    "    max_gain = -1\n",
    "    max_params = None\n",
    "    max_attr = None\n",
    "    for attr in possible_attributes:\n",
    "        gain, params = get_info_gain_for_attr(dataset, prev_entropy, attr, target)\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            max_params = params\n",
    "            max_attr = attr\n",
    "    \n",
    "    node = TreeNode(max_attr)\n",
    "    children = []\n",
    "    for value, dic in max_params.iteritems():\n",
    "        children.append(get_attr_to_split(dic['dataset'], target, dic['entropy']))\n",
    "    node.set_children(children)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_entropy = get_entropy(dataset, 'Enjoy')\n",
    "get_attr_to_split(dataset, 'Enjoy', prev_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, attr_name):\n",
    "        self.children = None\n",
    "        self.name = attr_name\n",
    "        self.class_ = None\n",
    "    \n",
    "    def set_children(self, children):\n",
    "        self.children = children\n",
    "    \n",
    "    def set_class(self, class_):\n",
    "        self.class_ = class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = TreeNode()\n",
    "tree.fit(dataset, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cls = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "one_hot = OneHotEncoder()\n",
    "pipeline = Pipeline([('encoder', one_hot), ('classifier', cls)])\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "X = np.array([encoder.fit_transform(column) for column in dataset.drop('Enjoy', 1).values.T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, dataset['Enjoy'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(pipeline.named_steps.classifier, out_file='tree.dot')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree.dot -o tree.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.figure(figsize=(12,18))\n",
    "plt.axis('off')\n",
    "img=mpimg.imread('tree.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = dataset.drop('Enjoy', 1).columns\n",
    "ax = 0\n",
    "for j in range(len(columns)):\n",
    "    column = columns[j]\n",
    "    encoder.fit_transform(dataset[column])\n",
    "    #print \"x_\"+str(j)+\": \"+column\n",
    "    for i in range(len(encoder.classes_)):\n",
    "        print \"x_\"+str(ax)+\": \"+column+\"_\"+encoder.classes_[i]\n",
    "        ax += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
